{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfbff90d-bf0a-454a-969e-a1be13a33002",
   "metadata": {},
   "source": [
    "# Analysis report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dfdb81-772e-4629-a669-12c08f4fdfbe",
   "metadata": {},
   "source": [
    "## EDA & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd33420-08ca-4749-b764-1976c1181970",
   "metadata": {},
   "source": [
    "*Code and outputs documented in exploration.ipynb*\n",
    "\n",
    "* Dataset to be analyzed was bigquery-public-data.google_analytics_sample. It consists of 366 tables, each of which contains analytics data for one day for googlemerchandisestore.com between August 2016 and July 2017.\n",
    "* Chose March 2017 as the month from which to design the training set. Since a one-month period was specified for this purpose, decided to choose a month that would avoid anomalies associated with holiday shopping.\n",
    "* Used a wildcard table to query all of the March 2017 tables together. Quantified number of sessions (top-level rows), number of hits (our layer of interest and location of the target) and number of times the target appeared. This showed a class imbalance of about 96% target-negative rows.\n",
    "* Viewed a small sample of the data in a DataFrame to get a sense of field datatypes and the locations of repeated records\n",
    "\n",
    "* Wrote an exploratory query to unnest all records in the month's worth of data and save it to a DataFrame to facilitate reviewing and selecting features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34415a0b-6d41-42a1-91b3-9093815c0515",
   "metadata": {},
   "source": [
    "## Feature Selection & Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e22d5b-ad77-4579-9dfa-a8711947ee57",
   "metadata": {},
   "source": [
    "*Code and outputs documented in exploration.ipynb and ingest.ipynb*\n",
    "\n",
    "* Fully unnesting the data produced a total of 302 columns. First step was to drop all columns that were completely empty, after which 144 columns remained.\n",
    "\n",
    "* Found target by searching the Universal Analytics BigQuery export schema for mentions of fields related to an 'add to cart' action. It seems to have been doubly recorded, as a string in hits.eventInfo.eventAction and as a (string-type) numeral in hits.eCommerceAction.action_type. I used the latter to engineer the target column, which I specified to contain a 1 in rows where the action type corresponded to \"add to cart\", and a 0 in all other fields.\n",
    "* Even though an ecommerce action type of 0 corresponds to \"unknown\" and such rows accounted for at least 70% of the data, I left them in as I still considered such rows to be part of the group (hits) from which we were trying to predict the target's occurrence.\n",
    "* Reviewed all fields that were partially null. I dropped another set of fields which were almost completely null or very high-cardinality categorical that I couldn't see a straightforward way to encode.\n",
    "* Targeted and dropped the fields that were explicitly obfuscated. \n",
    "* Attempted to manually review the unique values and their relative frequencies in the remaining fields. This revealed a handful of zero-variance or redundant fields I was able to drop.\n",
    "* Identified a handful of categorical fields that I suspected would yield information on the target but were too high-cardinality to use directly. Decided to transform these features by keeping the most commonly appearing values in place and transforming the rest to 'other'. By this point the DataFrame was down to 46 columns.\n",
    "* After investigating some of the GCP model training options, re-added the product sku field in the hope this would be a one-feature way to insert a lot of product information into the model. Planned to use label encoding on this 800-value field if a better solution couldn't be found.\n",
    "* Engineered sku feature so that a value would only be inserted in rows where exactly one product was associated with the hit, to prevent spurious duplication of hits\n",
    "* Finally I dropped rows that indicated bounces in an effort to reduce the class imbalance slightly; not sure if this was advisable or not.\n",
    "* I wish I had decided earlier in the process that I would as a rule exclude any column from a repeated record **other** than hits, in order to avoid erroneously duplicating individual hits. The exception was sku which came from the product record, with the aforementioned engineering to prevent row duplcation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f0216a-c1f1-4896-88fc-e0ab351fa64b",
   "metadata": {},
   "source": [
    "## Model Choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f25f0b-477c-4030-9b25-e61c78146fbb",
   "metadata": {},
   "source": [
    "*Model generation queries are documented in ingest.ipynb*\n",
    "\n",
    "* Decided to try two versions each of two model types: XGBoost and Logistic Regression\n",
    "\n",
    "* These models are well suited to binary classification tasks and are readily available for use in GCP\n",
    "* For XGBoost I specified label encoding for the category encoding method, although upon reviewing the model's metadata afterward it appears that one-hot was actually used on all categorical features. Would like to investigate further why this happened and if it impacted performance\n",
    "* Tried two versions of an XGBoost model: one with automatic class weights applied and one without. Given the highly imbalanced label classes it was interesting to compare the presence or absence of this adjustment\n",
    "* The logistic regression model also used automatic weights, and there were also two versions here: one with the same number of trials as the XGBoost attempts, and one with double the trials and half the parallel trials.\n",
    "* I omitted the sku field from the logistic regression models because they do not allow specifying label encoding.\n",
    "* I went with the Vizier hyperparameter tuning algorithm for all versions as I thought it would be much more time-efficient than adjusting hyperparameters manually.\n",
    "* I left the default hparam tuning objective as roc-auc; it's a suitable metric for binary classification and is also useful in cases of label class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec6d0c7-a51b-4743-99ad-4c3e301ba91e",
   "metadata": {},
   "source": [
    "## Model Tuning & Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bd35ba-e570-4628-a374-4d6378710270",
   "metadata": {},
   "source": [
    "*Full metrics are documented in eval.ipynb*\n",
    "\n",
    "* Due to an accidental overwrite, model 4 was trained before model 3 and their optimal trial results are now identical. I suspect this is due to the transfer learning feature of the Vizier algorithm.\n",
    "* Model 1 was the only model trained without class weights. Its precision score (50%) was much higher than its recall score (15%).\n",
    "* Conversely, models 2 and 3 strongly favored recall in their performance.\n",
    "* The best balance between precision and recall came from model 4, which was distinguished by an increased number of trials.\n",
    "* Model 4 also used a lower number of max parallel trials, which may have enabled the hyperparameter search algorithm to improve it further.\n",
    "\n",
    "* The HP search space for both XGBoost models was:  \n",
    "Learn rate: [1, 10]  \n",
    "L1 regularization: [0, 10]  \n",
    "L2 regularization: [0, 10]  \n",
    "Max tree depth: [1, 10]  \n",
    "Subsample: [1e-14, 1]\n",
    "\n",
    "* The HP search space for both logistic regression models was:  \n",
    "L1 regularization: [0, 10]  \n",
    "L2 regularization: [0, 10]  "
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m115",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m115"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
